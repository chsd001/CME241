{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP:\n",
    "    def __init__(self,transition):\n",
    "        self.transition = transition\n",
    "    def computeStationnary(self):\n",
    "        Ker = self.transition.transpose() - np.identity(self.transition.shape[0])\n",
    "        values, vectors = np.linalg.eig(Ker)\n",
    "        zeroIndex = np.argmin(abs(values))\n",
    "        zeroVect = vectors[:,zeroIndex]\n",
    "        return zeroVect/np.sum(zeroVect)\n",
    "\n",
    "    \n",
    "class MRP(MP):\n",
    "    def __init__(self, transition, reward, gamma):\n",
    "        super().__init__(transition)\n",
    "        self.reward = reward\n",
    "        self.gamma = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRPValue(mrp):\n",
    "    return np.dot(np.linalg.inv(np.identity(mrp.transition.shape[0]) - mrp.gamma*mrp.transition), mrp.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromDeterministic(policy):\n",
    "    return np.argmax(policy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP():\n",
    "    def __init__(self, gamma, transition=None, reward=None,  mrp_list=None):\n",
    "        self.gamma = gamma\n",
    "        if mrp_list != None:\n",
    "            self.mrps = [MRP(transition[i], reward[i], gamma) for i in range(transition.shape[0])]\n",
    "        else:\n",
    "            self.mrps = mrp_list\n",
    "        self.S = self.mrps[0].transition.shape[0]\n",
    "        self.A = len(self.mrps)\n",
    "            \n",
    "    def toMRP(self, policy):\n",
    "    A, S = policy.shape\n",
    "    transition = np.zeros((S,S))\n",
    "    reward = np.zeros(S)\n",
    "    for s in range(S):\n",
    "        proba = 0\n",
    "        r = 0\n",
    "        for a in range(A):\n",
    "            r += policy[a, s] * self.mrps[a].reward[s]\n",
    "            proba += policy[a, s]*self.mrps[a].transition[s]\n",
    "        transition[s] = proba\n",
    "        reward[s] = r\n",
    "    return MRP(transition, reward, self.gamma)\n",
    "\n",
    "    def evaluation(policy):\n",
    "        return MRPValue(toMRP(self, policy))\n",
    "    \n",
    "    def iteration(start=np.zeros(self.S)):\n",
    "        value = np.zeros(self.S)\n",
    "        policy = start\n",
    "        output = np.zeros(self.S)\n",
    "        eq = False\n",
    "        while(!eq):\n",
    "            dv = self.evaluation(fromDeterministic(policy)) - value\n",
    "            if dv.sum() == 0:\n",
    "                eq = False\n",
    "                continue\n",
    "            for s in range(self.S):\n",
    "                old = start[s]\n",
    "                best = start[s]\n",
    "                maxVal = value[s]\n",
    "                for a in range(self.A):\n",
    "                    start[s] = a\n",
    "                    newVal = self.evaluation(fromDeterministic)[s]\n",
    "                    if newVal > maxVal:\n",
    "                        newVal = maxVal\n",
    "                        best = a\n",
    "                output[s] = best\n",
    "                start[s] = old\n",
    "            start = np.copy(output)\n",
    "        return start, value\n",
    "    \n",
    "    def valueIteration(start=0, eps):\n",
    "        value = np.zeros(self.S)\n",
    "        dv = np.inf\n",
    "        while(dv < eps):\n",
    "            prevValue = value.copy()\n",
    "            for s in range(self.S):\n",
    "                for a in range(self.A):\n",
    "                    val = self.mrps[a].reward[s] + np.sum(self.mrps[a].transition[s]*self.mrps[a].reward)\n",
    "                    if val > value[s]:\n",
    "                        value[s] = val\n",
    "            dv = np.sum(value-prevValue)\n",
    "        return value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self,policy):\n",
    "        self.policy = policy\n",
    "    def forward(self,s):\n",
    "        return np.random.choice(a = range(self.policy.shape[1]), p=self.policy[:,s])       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
