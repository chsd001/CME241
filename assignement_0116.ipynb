{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP:\n",
    "    def __init__(self,transition):\n",
    "        self.transition = transition\n",
    "    def computeStationnary(self):\n",
    "        Ker = self.transition.transpose() - np.identity(self.transition.shape[0])\n",
    "        values, vectors = np.linalg.eig(Ker)\n",
    "        zeroIndex = np.argmin(abs(values))\n",
    "        zeroVect = vectors[:,zeroIndex]\n",
    "        return zeroVect/np.sum(zeroVect)\n",
    "\n",
    "    \n",
    "class MRP(MP):\n",
    "    def __init__(self, transition, reward, gamma):\n",
    "        super().__init__(transition)\n",
    "        self.reward = reward\n",
    "        self.gamma = gamma\n",
    "          \n",
    "class MRP_2(MP):\n",
    "    def __init__(self, transition, reward, gamma):\n",
    "        super().__init__(transition)\n",
    "        self.reward = reward\n",
    "        self.gamma = gamma\n",
    "    def cast(self):\n",
    "        reward = np.diag(np.dot(self.reward, np.transpose(self.transition)))\n",
    "        return MRP_1(self.transition, reward, self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellman equation for an MRP value function $v$ can be expressed as\n",
    "\n",
    "<center>\n",
    "    <br>\n",
    "    $v=R + \\gamma pv$\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value(mrp):\n",
    "    return np.dot(np.linalg.inv(np.identity(mrp.transition.shape[0]) - mrp.gamma*mrp.transition), mrp.reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov Decision Process is defined by:\n",
    "\n",
    "   - A finite set of states $\\mathcal S$ \n",
    "   \n",
    "   - A finite set of actions $\\mathcal A$ \n",
    "\n",
    "   - A transition probability $\\mathcal P^{a}_{ss'}$ which gives the probability of moving from state $\\mathcal s$ to $\\mathcal {s'}$ after action $a$.\n",
    "   \n",
    "   - A reward function $\\mathcal R^{a}_s = \\mathbb E[R_{t+1}|S_{t}=\\mathcal s, A_t=\\mathcal a]$\n",
    "   \n",
    "   - A discount factor $\\gamma$ in $[0,1]$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A policy is a distribution of actions to take over the state space\n",
    "\n",
    "<center>\n",
    "    $\\pi(a|s) = \\mathbb P [A_t = a | S_t = s]$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function is the expected future return starting from state $\\mathcal s$ and following policy $\\pi$\n",
    "\n",
    "<center>\n",
    "    <br>\n",
    "    $v_\\pi(s) = E_\\pi[G_t| S_t = s]$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP():\n",
    "    def __init__(self, gamma, transition=None, reward=None,  mrp_list=None):\n",
    "        self.gamma = gamma\n",
    "        if mrp_list != None:\n",
    "            self.mrps = [MRP(transition[i], reward[i], gamma) for i in range(transition.shape[0])]\n",
    "        else:\n",
    "            self.mrps = mrp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self,policy):\n",
    "        self.policy = policy\n",
    "    def forward(self,s):\n",
    "        return np.random.choice(a = range(self.policy.shape[1]), p=self.policy[:,s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toMRP(mdp, policy):\n",
    "    A, S = policy.shape\n",
    "    transition = np.zeros((S,S))\n",
    "    reward = np.zeros(S)\n",
    "    for s in range(S):\n",
    "        proba = 0\n",
    "        r = 0\n",
    "        for a in range(A):\n",
    "            r += policy[a, s] * self.mrps[a].reward[s]\n",
    "            proba += policy[a, s]*mdp.mrps[a].transition[s]\n",
    "        transition[s] = proba\n",
    "        reward[s] = r\n",
    "    return MRP(transition, reward, mdp.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP_2():\n",
    "    def __init__(self, transition, reward, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.mrps = [MRP_2(transition[i], reward[i], gamma) for i in range(transition.shape[0])]\n",
    "    def cast():\n",
    "        return MDP(self.gamma, mrp_list = [mrp.cast() for mrp in self.mrps])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
